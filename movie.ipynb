{"cells":[{"cell_type":"code","source":["# Set up AWS S3 access credentials\nACCESS_KEY = \"\"\nSECRET_KEY = \"\"\nENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\nAWS_BUCKET_NAME = \"comp4651-movie-data\""],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["# Convert csv file to Spark DataFrame\ndef loadDataFrame(fileName, fileSchema):\n  return (spark.read.format(\"csv\")\n                    .schema(fileSchema)\n                    .option(\"header\", \"true\")\n                    .option(\"mode\", \"DROPMALFORMED\")\n                    .csv(\"s3a://%s:%s@%s/%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME, fileName)))"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["from pyspark.sql.types import *\n\nmovieRatingSchema = StructType([\n    StructField(\"userId\", IntegerType(), True),\n    StructField(\"movieId\", IntegerType(), True),\n    StructField(\"rating\", FloatType(), True),\n    StructField(\"timestamp\", StringType(), True)])\n\nmovieSchema = StructType([\n    StructField(\"movieId\", IntegerType(), True),\n    StructField(\"title\", StringType(), True),\n    StructField(\"genres\", StringType(), True)])\n\nsmallMovieRatingsDF = loadDataFrame(\"ratings-small.csv\", movieRatingSchema).cache()\nsmallMoviesDF = loadDataFrame(\"movies-small.csv\", movieSchema).cache()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Print out the DataFrame shcema, and a few lines as example\nsmallMovieRatingsDF.printSchema()\nsmallMovieRatingsDF.show(5)\n\nsmallMoviesDF.printSchema()\nsmallMoviesDF.show(5)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["from pyspark.sql.functions import mean, min, max, stddev\n\n# Data summary of the dataset\nprint \"Number of ratings: %s\" % (smallMovieRatingsDF.count())\nprint \"Number of distinct users: %s\" % (smallMovieRatingsDF.select('userId').distinct().count())\nprint \"Number of distinct movies: %s\" % (smallMovieRatingsDF.select('movieId').distinct().count())\nsmallMovieRatingsDF.select([mean('rating'), min('rating'), max('rating'), stddev('rating')]).show()\nsmallMovieRatingsDF.groupBy('rating').count().orderBy('rating').show()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# Partition the dataset into traning, validation and testing for cross-validation\n(trainingSet, validationSet, testingSet) = smallMovieRatingsDF.randomSplit([0.6, 0.2, 0.2], seed=12345)\ntraining = trainingSet.cache()\nvalidation = validationSet.cache()\ntesting = testingSet.cache()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# Use ml instead of mlib for Dataframes\n# http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.recommendation.ALS\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.sql import Row\n\nranks = [2, 4, 8, 12, 16, 20, 24]\nregParams = [0.01, 0.05, 0.1, 0.15, 0.2, 0.3]\nminError = float('inf')\nbestRank = -1\nbestRegParam = -1\nbestModel = None\n\n# An RMSE evaluator using the rating and predicted rating columns\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n# Initialize the ASL(Alternating Least Squares)\nals = ALS(userCol = \"userId\", itemCol = \"movieId\", ratingCol = \"rating\", seed = 123)\n\nfor regParam in regParams:\n  for rank in ranks:\n    # Build the recommendation model using ALS on the training data\n    als.setParams(rank = rank, regParam = regParam)\n    model = als.fit(training)\n\n    # Evaluate the model by computing the RMSE on the validation data\n    predictions = model.transform(validation)\n    predictions = predictions.dropna() # drop all NaN prediction value to ensure not to have NaN RMSE (due to SPARK-14489)\n    error = evaluator.evaluate(predictions)\n    \n    if error < minError:\n      bestRank = rank\n      bestRegParam = regParam\n      minError = error\n      bestModel = model\n    print 'For rank %s, regParams %s, the RMSE is %s' % (rank, regParam, error)\n\nprint(\"Best Rank = %s, Best regParam = %s, with RMSE = %s\"  % (bestRank, bestRegParam, minError))"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# After getting the best rank and RegParam, test the model on test dataset\npredictions = bestModel.transform(testing)\npredictions = predictions.dropna() # drop all NaN prediction value to ensure not to have NaN RMSE (due to SPARK-14489)\nrmse = evaluator.evaluate(predictions)\nprint(\"The model had a RMSE of %s on test dataset\"  % (rmse))"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# Train the full data set and calculate the time elapsed\nMovieRatingsDF = loadDataFrame(\"ratings.csv\", movieRatingSchema).cache()\nMoviesDF = loadDataFrame(\"movies.csv\", movieSchema).cache()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# Data summary of the full dataset on movie rating\nprint \"Number of ratings: %s\" % (MovieRatingsDF.count())\nprint \"Number of distinct users: %s\" % (MovieRatingsDF.select('userId').distinct().count())\nprint \"Number of rated distinct movies: %s\" % (MovieRatingsDF.select('movieId').distinct().count())\nprint \"Total number of movies: %s\" % (MoviesDF.select('movieId').count())\n\nMovieRatingsDF.select([mean('rating'), min('rating'), max('rating'), stddev('rating')]).show()\n\nprint \"Distribution of ratings:\"\nMovieRatingsDF.groupBy('rating').count().orderBy('rating').show()\nRatingsCountGroupByMovieId = MovieRatingsDF.groupBy('movieId').count()\nprint \"Average number of ratings per movie: %s\" % (RatingsCountGroupByMovieId.select(mean('count')).first())"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["from time import time\n\nals.setParams(rank = bestRank, regParam = bestRegParam)\nprint \"Training full data set with Rank = %s, regParam = %s ...\" % (bestRank, bestRegParam)\n\ntimeBegin = time()\n\nmodel = als.fit(MovieRatingsDF) # use the full dataset for training\n\ntimeElapsed = time() - timeBegin\n\nprint \"Final model trained in %s seconds\" % round(timeElapsed, 2)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# Evaluate the performance of the final model with the testing data\npredictions = model.transform(testing)\npredictions = predictions.dropna() # drop all NaN prediction value to ensure not to have NaN RMSE (due to SPARK-14489)\nrmse = evaluator.evaluate(predictions)\nprint(\"The final model had a RMSE of %s\"  % (rmse))"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["from pyspark.sql.functions import lit\nUserId = 1000\nuserWatchedList = MovieRatingsDF.filter(MovieRatingsDF.userId == UserId).join(MoviesDF, 'movieId').select(['movieId', 'userId', 'title', 'rating'])\nwatchedMovieList = []\nfor movie in userWatchedList.collect():\n  watchedMovieList.append(movie.movieId)\nprint \"User %s has watched and rated %s moive (sorted by rating):\" % (UserId, len(watchedMovieList)) \nuserWatchedList.orderBy('rating', ascending = False).show(20, False)\n\n# find out the unwatched list and append with the userid\nuserUnwatchedList = MoviesDF.filter(MoviesDF.movieId.isin(watchedMovieList) == False).withColumn('userId', lit(UserId)).cache()\nprint \"%s unwatched movie:\" % (userUnwatchedList.count())\nuserUnwatchedList.show(20, False)\n\npredictedMovies = model.transform(userUnwatchedList)\npredictedMovies = predictedMovies.dropna().cache() # drop all NaN prediction value to ensure not to have NaN RMSE (due to SPARK-14489)\n"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["print \"Top 25 predicted movie with highest rating:\"\ntop25Movies = predictedMovies.orderBy('prediction', ascending = False).show(25, False)\n\nprint \"Top 25 commedy with highest rating:\"\ntop25Comedy = predictedMovies.filter(predictedMovies.genres.like(\"%Comedy%\")).orderBy('prediction', ascending = False).show(25, False)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["N = 20\nMovieWithLessThanNRatings = RatingsCountGroupByMovieId.filter('count <' + str(N))\nprint \"Movies with less than %s rating count: %s\" % (N, MovieWithLessThanNRatings.count())\n\nmovieToBeExcluded = []\nfor movie in MovieWithLessThanNRatings.collect():\n  movieToBeExcluded.append(movie.movieId)\n  \nuserUnwatchedListWithAtLeastNRatings = userUnwatchedList.filter(userUnwatchedList.movieId.isin(movieToBeExcluded) == False).cache()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["predictedMovies = model.transform(userUnwatchedListWithAtLeastNRatings)\npredictedMovies = predictedMovies.dropna().cache() # drop all NaN prediction value to ensure not to have NaN RMSE (due to SPARK-14489)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["print \"Top 25 predicted movie with highest rating:\"\ntop25Movies = predictedMovies.orderBy('prediction', ascending = False).show(25, False)\n\nprint \"Top 25 commedy with highest rating:\"\ntop25Comedy = predictedMovies.filter(predictedMovies.genres.like(\"%Comedy%\")).orderBy('prediction', ascending = False).show(25, False)\n\nprint \"Top 25 Science Fiction with highest rating:\"\ntop25SciFi = predictedMovies.filter(predictedMovies.genres.like(\"%Sci-Fi%\")).orderBy('prediction', ascending = False).show(25, False)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# Save the trained model to S3\nmodel.save(\"s3a://%s:%s@%s/%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME, \"model\"))"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# Load model previous saved model\nfrom pyspark.ml.recommendation import ALSModel\nmodel = ALSModel.load(\"s3a://%s:%s@%s/%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME, \"model\"))"],"metadata":{},"outputs":[],"execution_count":19}],"metadata":{"name":"movie-test","notebookId":3619855571353653},"nbformat":4,"nbformat_minor":0}
