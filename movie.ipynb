{"cells":[{"cell_type":"markdown","source":["## Build a Movie Recommendation Model with Apache Spark and Amazon EMR\n\n**COMP4651 2017 Spring Project**\n\n*O Pui Wai, opw0011 | CHAN Hon Sum, samchan1995 | TSUI Ka Wai, kwtsui *\n\n<img src=\"https://raw.githubusercontent.com/hortonworks/data-tutorials/1f3893c64bbf5ffeae4f1a5cbf1bd667dcea6b06/tutorials/hdp/hdp-2.6/intro-to-machine-learning-with-apache-spark-and-apache-zeppelin/assets/spark-mllib-logo.png\" align=\"left\" height=\"100\" >\n<img src=\"https://conceptdraw.com/a3131c3/p1/preview/640/pict--amazon-emr-aws-analytics-vector-stencils-library\" width=\"140\">\n\n\n### Introduction\n\nIn this notebook, we are going to train a movie recommendation model using Spark MLib, a Spark's machine learning library . With the trained model, we can predict the movie ratings that a user is likely to give. As a result, we can make tailor-made moive recommendations based on the predicted rating, such listing out the top 25 highest rating moives that a user is likely to watch.   \n\n\n### Project Setup\n\nThe dataset is placed on AWS S3. In order to access the files, fill in the `ACCESS_KEY`, `SECRETE_KEY` and `AWS_BUCKET_NAME`."],"metadata":{}},{"cell_type":"code","source":["# Set up AWS S3 access credentials\nACCESS_KEY = \"FILL_IN_YOUR_KEY_HERE\"\nSECRET_KEY = \"FILL_IN_YOUR_KEY_HERE\"\nENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\nAWS_BUCKET_NAME = \"comp4651-movie-data\""],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["#### Dataset Preparation\n\nThe dataset that is used for traning the model is downloaded from [MovieLens](https://grouplens.org/datasets/movielens/), a movie recommendation service.\n\nWe have downloaded two set the data:\n\nSmall: 100,000 ratings and 1,300 tag applications applied to 9,000 movies by 700 users. (compressed size: 1 MB) \n\n- [ml-latest-small.zip](http://files.grouplens.org/datasets/movielens/ml-latest-small.zip) (Last updated 10/2016)\n\nFull: 24,000,000 ratings and 670,000 tag applications applied to 40,000 movies by 260,000 users. (compressed size: 224 MB)\n- [ml-latest.zip](http://files.grouplens.org/datasets/movielens/ml-latest.zip) (Last updated 10/2016)\n\nThen, we have uploaded all the files to AWS S3:\n\n![s3](https://cloud.githubusercontent.com/assets/10897048/26273579/459ee37a-3d65-11e7-8460-215adf4e7335.png)\n\nSince the raw data are in csv format, we need to write a function to convert the raw data into Spark DataFrame."],"metadata":{}},{"cell_type":"code","source":["# Convert csv file to Spark DataFrame (Databricks version)\ndef loadDataFrame(fileName, fileSchema):\n  return (spark.read.format(\"csv\")\n                    .schema(fileSchema)\n                    .option(\"header\", \"true\")\n                    .option(\"mode\", \"DROPMALFORMED\")\n                    .csv(\"s3a://%s:%s@%s/%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME, fileName)))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["#### Dataset Summary\n\n- `ratings-small.csv` is a small dataset extracted from the full rating dataset. It consists of the users' movie ratings, and has the following format:\n\n - `userId,movieId,rating,timestamp`\n\n- `movies-small.csv` is  a small dataset extracted from the full movie dataset. It consists of the details of the moive, and has the following format:\n\n - `movieId,title,genres`\n\nIn order to parse the csv into DataFrame, we have to explicitly specify the data type of each field."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\n\nmovieRatingSchema = StructType([\n    StructField(\"userId\", IntegerType(), True),\n    StructField(\"movieId\", IntegerType(), True),\n    StructField(\"rating\", FloatType(), True),\n    StructField(\"timestamp\", StringType(), True)])\n\nmovieSchema = StructType([\n    StructField(\"movieId\", IntegerType(), True),\n    StructField(\"title\", StringType(), True),\n    StructField(\"genres\", StringType(), True)])\n\nsmallMovieRatingsDF = loadDataFrame(\"ratings-small.csv\", movieRatingSchema).cache()\nsmallMoviesDF = loadDataFrame(\"movies-small.csv\", movieSchema).cache()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["After successfully created the Dataframe, we can verify it by printing the DataFrame schema. \n\nAlso, we can write some code using the [Spark DataFrame API](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame) to get a more detailed dataset summary."],"metadata":{}},{"cell_type":"code","source":["# Print out the DataFrame shcema, and a few lines as example\nsmallMovieRatingsDF.printSchema()\nsmallMovieRatingsDF.show(5)\n\nsmallMoviesDF.printSchema()\nsmallMoviesDF.show(5)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["from pyspark.sql.functions import mean, min, max, stddev\n\n# Data summary of the dataset\nprint \"Number of ratings: %s\" % (smallMovieRatingsDF.count())\nprint \"Number of distinct users: %s\" % (smallMovieRatingsDF.select('userId').distinct().count())\nprint \"Number of distinct movies: %s\" % (smallMovieRatingsDF.select('movieId').distinct().count())\nsmallMovieRatingsDF.select([mean('rating'), min('rating'), max('rating'), stddev('rating')]).show()\nsmallMovieRatingsDF.groupBy('rating').count().orderBy('rating').show()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["### Model Training\n\n#### Overview\n\nMLlib is Sparkâ€™s machine learning (ML) library. It provides high-level machine learning API to develop scalable machine learning program easily.\n\nSpark MLLib provides two machine learning packages, namely `spark.mllib` and `spark.ml`.\n\n`spark.mllib` consists of RDD-based APIs that supports RDD as the input parameters. However, it is expected that in Spark 2.2, the RDD-based API will be deprecated.\n\nOn the contrary, the DataFrame-based APIs in `spark.ml` is the latest trend in Spark, because DataFrames provide a more user-friendly API than RDDs. Besides, it has better execution performance, such as the Catalyst optimizations.\n\nFor more detailed explanations, please visit [Spark's offical MLlib guide](https://spark.apache.org/docs/2.1.0/ml-guide.html).\n\nAs suggested by Spark offical programming guide, we will mainly use the DataFrame-based [pyspark.ml.recommendation module](https://spark.apache.org/docs/2.1.0/api/python/pyspark.ml.html#module-pyspark.ml.recommendation) for doing collaborative filtering.\n\n#### Collaborative Filtering\n\nCoolaborative filtering make predictions about a user's interests by collecting preferences information from many users. It is based on the assumption that if a user A has the same opinion as a user B on an issue, then A is more likely to have B's opinion on a different issue. The image below illustrates this idea:\n\n![collaborative Filtering](https://upload.wikimedia.org/wikipedia/commons/5/52/Collaborative_filtering.gif) \n\n*(Image from [Wikipedia](https://en.wikipedia.org/wiki/Collaborative_filtering))*\n\nIn Spark ML library, it uses Alternating Least Squares (ALS) matrix factorization to perform recommendation. \n\n- rank is the number of latent factors in the model. In other words, it referes to the number of hidden factors of the real model.\n\n- regParam specifies the regularization parameter in ALS.\n\nOf course we do not know how many underlying factors for the real model, so we have to make a reasonalble guess on those paramters, and choose the best combination after doing evaluations.  \n\n\n#### Pick the Best ALS Parameters with Samll Dataset\n\nIn order to determine a good combination of model training parameters, we will split the small dataset into 3 non-overlapping subsets, namely traning (60%), validation (20%) and testing (20%), with [randomSplit](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit).\n\nThen, we have designed two list of parameters that will be used for traning:\n```\nranks = [2, 4, 8, 12, 16, 20, 24]\nregParams = [0.01, 0.05, 0.1, 0.15, 0.2, 0.3]\n```\nWe are going to train multiple models based on the training set. Then, we will pick the best model on the validation set with the least RMSE (Root Mean Squared Error). Finally, we will evaluate the best model on the testing set."],"metadata":{}},{"cell_type":"code","source":["# Partition the dataset into traning, validation and testing for cross-validation\n(trainingSet, validationSet, testingSet) = smallMovieRatingsDF.randomSplit([0.6, 0.2, 0.2], seed=12345)\ntraining = trainingSet.cache()\nvalidation = validationSet.cache()\ntesting = testingSet.cache()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# Use ml instead of mlib for Dataframes\n# http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.recommendation.ALS\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.sql import Row\nfrom time import time\n\nranks = [2, 4, 8, 12, 16, 20, 24]\nregParams = [0.01, 0.05, 0.1, 0.15, 0.2, 0.3]\nminError = float('inf')\nbestRank = -1\nbestRegParam = -1\nbestModel = None\n\n# An RMSE evaluator using the rating and predicted rating columns\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n# Initialize the ASL(Alternating Least Squares)\nals = ALS(userCol = \"userId\", itemCol = \"movieId\", ratingCol = \"rating\", seed = 12345)\n\ntimeBegin = time()\n\nfor regParam in regParams:\n  for rank in ranks:\n    # Build the recommendation model using ALS on the training data\n    als.setParams(rank = rank, regParam = regParam)\n    model = als.fit(training)\n\n    # Evaluate the model by computing the RMSE on the validation data\n    predictions = model.transform(validation)\n    predictions = predictions.dropna() # drop all NaN prediction value to ensure not to have NaN RMSE (due to SPARK-14489)\n    error = evaluator.evaluate(predictions)\n    \n    if error < minError:\n      bestRank = rank\n      bestRegParam = regParam\n      minError = error\n      bestModel = model\n    print 'For rank %s, regParams %s, the RMSE is %s' % (rank, regParam, error)\n\ntimeElapsed = time() - timeBegin\nprint \"Trained %s models in %s seconds\" % (len(ranks) * len(regParams), round(timeElapsed, 3))\nprint(\"Best Rank = %s, Best regParam = %s, with RMSE = %s\"  % (bestRank, bestRegParam, minError))"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# After getting the best rank and RegParam, test the model on test dataset\npredictions = bestModel.transform(testing)\npredictions = predictions.dropna() # drop all NaN prediction value to ensure not to have NaN RMSE (due to SPARK-14489)\nrmse = evaluator.evaluate(predictions)\nprint(\"The model had a RMSE of %s on test dataset\"  % (rmse))"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["#### Train the Model with Full Dataset\n\nAfter we have found out the best training parameters with the samll dataset, now we are going to use them for training the full dataset (~600MB in size).\n\nDuring the traning, we will record the time spent to train the whole dataset."],"metadata":{}},{"cell_type":"code","source":["# Train the full data set and calculate the time elapsed\nMovieRatingsDF = loadDataFrame(\"ratings.csv\", movieRatingSchema).cache()\nMoviesDF = loadDataFrame(\"movies.csv\", movieSchema).cache()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# Data summary of the full dataset on movie rating\nprint \"Number of ratings: %s\" % (MovieRatingsDF.count())\nprint \"Number of distinct users: %s\" % (MovieRatingsDF.select('userId').distinct().count())\nprint \"Number of rated distinct movies: %s\" % (MovieRatingsDF.select('movieId').distinct().count())\nprint \"Total number of movies: %s\" % (MoviesDF.select('movieId').count())\n\nMovieRatingsDF.select([mean('rating'), min('rating'), max('rating'), stddev('rating')]).show()\n\nprint \"Distribution of ratings:\"\nMovieRatingsDF.groupBy('rating').count().orderBy('rating').show()\nRatingsCountGroupByMovieId = MovieRatingsDF.groupBy('movieId').count()\nprint \"Average number of ratings per movie: %s\" % (RatingsCountGroupByMovieId.select(mean('count')).first())"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["als.setParams(rank = bestRank, regParam = bestRegParam)\nprint \"Training full data set with Rank = %s, regParam = %s ...\" % (bestRank, bestRegParam)\n\ntimeBegin = time()\n\nmodel = als.fit(MovieRatingsDF) # use the full dataset for training\n\ntimeElapsed = time() - timeBegin\n\nprint \"Final model trained in %s seconds\" % round(timeElapsed, 3)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# Evaluate the performance of the final model with the testing data\npredictions = model.transform(testing)\npredictions = predictions.dropna() # drop all NaN prediction value to ensure not to have NaN RMSE (due to SPARK-14489)\nrmse = evaluator.evaluate(predictions)\nprint(\"The final model had a RMSE of %s\"  % (rmse))"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["### Moive Recommendations\n\nAfter we have trained the model, now it is time to use the model to predict users' movie ratings.\n\nThe high level idea is like that:\n\n1. We pick a user, and get a list of movie that he/she has rated.\n\n2. From the full movie list, we exclude all the moive that the user has rated, and construct a list of un-rated movie list.\n\n3. The movies in the un-rated list are now in the pool of recommendation. They are then inputed into the model.\n\n4. Now, each un-rated movies will get a unique predicted rating based other similar users' ratings. \n\n5. Finally, we simply do a sorting to find the top N moives with the highest predicted rating. Besides, we can also make recommendation based on moive's categories.\n\n#### Our First Try\n\nWe follow the idea illustrated above and pick a user with UserId = 1000, and try to make a movie recommendation to him/her.\n\nIn the following example, we try to print out some movies that he/she has and has not watched before respectively, and then make predictions on all the unwatched movies."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import lit\nUserId = 1000\nuserWatchedList = (MovieRatingsDF.filter(MovieRatingsDF.userId == UserId)\n                                 .join(MoviesDF, 'movieId')\n                                 .select(['movieId', 'userId', 'title', 'rating']))\nwatchedMovieList = []\nfor movie in userWatchedList.collect():\n  watchedMovieList.append(movie.movieId)\nprint \"User %s has watched and rated %s moive (sorted by rating):\" % (UserId, len(watchedMovieList)) \nuserWatchedList.orderBy('rating', ascending = False).show(20, False)\n\n# find out the unwatched list and append with the userid\nuserUnwatchedList = (MoviesDF.filter(MoviesDF.movieId.isin(watchedMovieList) == False).withColumn('userId', lit(UserId)).cache())\nprint \"%s unwatched movie:\" % (userUnwatchedList.count())\nuserUnwatchedList.show(20, False)\n\npredictedMovies = model.transform(userUnwatchedList)\npredictedMovies = predictedMovies.dropna().cache() # drop all NaN prediction value to ensure not to have NaN RMSE (due to SPARK-14489)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["#### Our First Try Result\n\nAs you can see in the folliwng, we have list out the top 25 moives with the highest rating. \n\nHowever, we got some strange movies that have not be heard before. For example, take \"The Thorn (1971)\" as an example, try to search it on IMDB.\n\n![the-thorn](https://cloud.githubusercontent.com/assets/10897048/26272746/1e6cdc38-3d53-11e7-9483-554bf6b98e2d.png)\n\n**The problem is ONLY 48 people have rated this moive!**\n\nSince the sampling size is relatively too small, it may not be representative. There might be some biases on the ratings. Besides, a user may not be interested in watching unpopular moive. In order to fix this problem, we need to filter out those unknown movies."],"metadata":{}},{"cell_type":"code","source":["print \"Top 25 predicted movie with highest rating:\"\ntop25Movies = predictedMovies.orderBy('prediction', ascending = False).show(25, False)\n\nprint \"Top 25 commedy with highest rating:\"\ntop25Comedy = (predictedMovies.filter(predictedMovies.genres.like(\"%Comedy%\"))\n                              .orderBy('prediction', ascending = False)\n                              .show(25, False))"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["#### Our Second Try\n\nThis time we will filter out movies with less than 50 ratings, and then exclude them in the recommendation pool. We have excluded 27575 moives with less than 50 ratings."],"metadata":{}},{"cell_type":"code","source":["N = 50\nMovieWithLessThanNRatings = RatingsCountGroupByMovieId.filter('count <' + str(N))\nprint \"Movies with less than %s rating count: %s\" % (N, MovieWithLessThanNRatings.count())\n\nmovieToBeExcluded = []\nfor movie in MovieWithLessThanNRatings.collect():\n  movieToBeExcluded.append(movie.movieId)\n  \nuserUnwatchedListWithAtLeastNRatings = userUnwatchedList.filter(userUnwatchedList.movieId.isin(movieToBeExcluded) == False).cache()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["timeBegin = time()\n\npredictedMovies = model.transform(userUnwatchedListWithAtLeastNRatings)\npredictedMovies = predictedMovies.dropna().cache() # drop all NaN prediction value to ensure not to have NaN RMSE (due to SPARK-14489)\n\ntimeElapsed = time() - timeBegin\nprint \"Final model make prediction in %s seconds\" % round(timeElapsed, 3)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["#### Our Second Try Result\n\nThis time you can see that in our top recommendations list, all those unpopular movies has been excluded. \n\nNow, the top recommendation is \"Planet Earth (2006)\", which looks to be a suprisingly good rating moive that worth to be watched! \n\n![planet-earth](https://cloud.githubusercontent.com/assets/10897048/26272914/a4750c62-3d56-11e7-9f90-7b4c0882376c.png)\n\nWe can also list out the top rating based on the moive categories. \n\n**Now we have successfully build a practical movide recommendation model!**"],"metadata":{}},{"cell_type":"code","source":["print \"Top 25 predicted movie with highest rating:\"\ntimeBegin1 = time()\ntop25Movies = predictedMovies.orderBy('prediction', ascending = False).show(25, False)\ntimeElapsed1 = time() - timeBegin1\nprint \"Took %s seconds\" % round(timeElapsed1, 3)\n\nprint \"Top 25 commedy with highest rating:\"\ntimeBegin2 = time()\ntop25Comedy = (predictedMovies.filter(predictedMovies.genres.like(\"%Comedy%\"))\n                              .orderBy('prediction', ascending = False)\n                              .show(25, False))\ntimeElapsed2 = time() - timeBegin2\nprint \"Took %s seconds\" % round(timeElapsed2, 3)\n\nprint \"Top 25 Science Fiction with highest rating:\"\ntimeBegin3 = time()\ntop25SciFi = (predictedMovies.filter(predictedMovies.genres.like(\"%Sci-Fi%\"))\n                             .orderBy('prediction', ascending = False)\n                             .show(25, False))\ntimeElapsed3 = time() - timeBegin3\nprint \"Took %s seconds\" % round(timeElapsed3, 3)\n\nprint \"Top 25 Animation with highest rating:\"\ntimeBegin4 = time()\ntop25Anim = (predictedMovies.filter(predictedMovies.genres.like(\"%Animation%\"))\n                             .orderBy('prediction', ascending = False)\n                             .show(25, False))\ntimeElapsed4 = time() - timeBegin4\nprint \"Took %s seconds\" % round(timeElapsed4, 3)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["### Persisting the model\n\nAfter we have trained the model, we may want to store it somewhere for later use, such as saving time when starting up the server. We can save it easily to AWS S3 by specifying the bucket name, access key, secrete key and the file path. \n\nThe following are examples for saving and loading the trained ALS model:"],"metadata":{}},{"cell_type":"code","source":["timeBegin = time()\n\n# Save the trained model to S3\nmodel.write().overwrite().save(\"s3a://%s:%s@%s/%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME, \"model\"))\n\ntimeElapsed = time() - timeBegin\nprint \"Save model took %s seconds\" % round(timeElapsed, 3)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["from pyspark.ml.recommendation import ALSModel\n\ntimeBegin = time()\n\n# Load model previous saved model\nmodel = ALSModel.load(\"s3a://%s:%s@%s/%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME, \"model\"))\n\ntimeElapsed = time() - timeBegin\nprint \"Load model took %s seconds\" % round(timeElapsed, 3)"],"metadata":{},"outputs":[],"execution_count":30}],"metadata":{"name":"movie-test","notebookId":3619855571353653},"nbformat":4,"nbformat_minor":0}
